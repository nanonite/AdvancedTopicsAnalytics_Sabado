{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiomora03/AdvancedTopicsAnalytics/blob/main/exercises/E4-SpamClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Skr49zlgwe5"
      },
      "source": [
        "# Exercise 3\n",
        "## Spam Classification\n",
        "### Context\n",
        "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\n",
        "\n",
        "### Content\n",
        "The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\n",
        "\n",
        "This corpus has been collected from free or free for research sources at the Internet:\n",
        "\n",
        "- A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: [Web Link](http://www.grumbletext.co.uk/).\n",
        "- A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: [Web Link](http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/).\n",
        "- A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis available at [Web Link](http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf).\n",
        "- Finally, we have incorporated the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages and it is public available at: [Web Link](http://www.esp.uem.es/jmgomez/smsspamcorpus/). This corpus has been used in the following academic researches:\n",
        "\n",
        "Acknowledgements\n",
        "The original dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). The creators would like to note that in case you find the dataset useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\n",
        "\n",
        "We offer a comprehensive study of this corpus in the following paper. This work presents a number of statistics, studies and baseline results for several machine learning methods.\n",
        "\n",
        "Almeida, T.A., GÃ³mez Hidalgo, J.M., Yamakami, A. Contributions to the Study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (DOCENG'11), Mountain View, CA, USA, 2011."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK8H8navE5Eo",
        "outputId": "c1fe71e8-1379-4e3b-ce06-d8f5d7beec6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=20db22d04371325242d52d6cc90be7eb384188ea4ed09cf4711359319b8ac4fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgef-TTiglKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ab0509-1bf9-4e55-818f-0ee325dfca3b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wget\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmxHMGIEeNAl",
        "outputId": "dcff2370-1eb3-4a73-be7d-3df816eda23d"
      },
      "source": [
        "try :\n",
        "    from google.colab import files\n",
        "    !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
        "    !unzip smsspamcollection.zip\n",
        "    df = pd.read_csv('SMSSpamCollection', sep='\\t',  header=None, names=['target', 'text'])\n",
        "except ModuleNotFoundError :\n",
        "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
        "    path = os.getcwd()+'\\Data'\n",
        "    wget.download(url,path)\n",
        "    temp=path+'\\smsspamcollection.zip'\n",
        "    file = ZipFile(temp)\n",
        "    file.extractall(path)\n",
        "    file.close()\n",
        "    df = pd.read_csv(path + '\\SMSSpamCollection', sep='\\t',  header=None, names=['target', 'text'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-20 20:47:50--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘smsspamcollection.zip’\n",
            "\n",
            "\rsmsspamcollection.z     [<=>                 ]       0  --.-KB/s               \rsmsspamcollection.z     [ <=>                ] 198.65K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-03-20 20:47:50 (1.83 MB/s) - ‘smsspamcollection.zip’ saved [203415]\n",
            "\n",
            "Archive:  smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hPwGRB3ghm7y",
        "outputId": "aa9654bc-78d6-4d9c-df2e-7387796962c7"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  target                                               text\n",
              "0    ham  Go until jurong point, crazy.. Available only ...\n",
              "1    ham                      Ok lar... Joking wif u oni...\n",
              "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3    ham  U dun say so early hor... U c already then say...\n",
              "4    ham  Nah I don't think he goes to usf, he lives aro..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b80ffb40-8da3-457b-b521-354b325e16a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b80ffb40-8da3-457b-b521-354b325e16a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b80ffb40-8da3-457b-b521-354b325e16a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b80ffb40-8da3-457b-b521-354b325e16a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c0e37d7b-df28-4050-a9a2-21737f1f6e6b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c0e37d7b-df28-4050-a9a2-21737f1f6e6b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c0e37d7b-df28-4050-a9a2-21737f1f6e6b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "rKpOnNz9hq-x",
        "outputId": "c0bc96f8-b797-4027-b3d3-d39be1209208"
      },
      "source": [
        "display(df.shape) #Number of rows (instances) and columns in the dataset\n",
        "df[\"target\"].value_counts()/df.shape[0] #Class distribution in the dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(5572, 2)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     0.865937\n",
              "spam    0.134063\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df ['target'].value_counts ()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TylntwTr8Ej",
        "outputId": "50ee4e2b-9e71-4662-98e0-6e2444313626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam     747\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['target'].map({'ham':0, 'spam':1})"
      ],
      "metadata": {
        "id": "2u_qu5GzGs7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['target'], test_size = 0.3, random_state = 18)"
      ],
      "metadata": {
        "id": "y6W-OsRvNJK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the text data by removing stop words, converting all text to lowercase, and removing punctuation using NLTK package.\n"
      ],
      "metadata": {
        "id": "hUJxVPloNs8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_test = X_test.apply(preprocess)"
      ],
      "metadata": {
        "id": "8D2zWvpcNv-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Word2Vec model on the preprocessed training data using Gensim package."
      ],
      "metadata": {
        "id": "hT3AWK7pOX5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [sentence.split() for sentence in X_train]\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, negative=20, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "Gg_4OYMeObaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the preprocessed text data to a vector representation using the Word2Vec model."
      ],
      "metadata": {
        "id": "ZJJPREbbOyon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(100)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])"
      ],
      "metadata": {
        "id": "Qsg3iEZ0Ox8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a classification model such as logistic regression, random forests, or support vector machines using the vectorised training data and the sentiment labels."
      ],
      "metadata": {
        "id": "w8wdim2yPY8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "Vxh6o9jgPOti",
        "outputId": "86808da8-dd75-472d-b232-f693b7397e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the performance of the classification model on the testing set with the accuracy, precision, recall and F1 score."
      ],
      "metadata": {
        "id": "Lpa4X7l7PdO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK5pgzC6Pd9y",
        "outputId": "9ca7a950-d64e-4a81-fb45-64a8f5a03f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8660287081339713\n",
            "AUC: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest Classification**"
      ],
      "metadata": {
        "id": "y7ticw7bVBkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clrf = RandomForestClassifier()\n",
        "clrf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "q-amrJskUS45",
        "outputId": "67092c32-4dd9-491a-ae4f-fd3ab6479c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clrf.predict(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3nBbr9yUYVh",
        "outputId": "59131136-705a-4e95-98b5-51fbe2ac881e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9204545454545454\n",
            "AUC: 0.7087855169692187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercise 3.1\n",
        "\n",
        "Remove stopwords, then predict target using CountVectorizer.\n",
        "\n",
        "use Random Forest classifier"
      ],
      "metadata": {
        "id": "bdkzBNohGAN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['target'], test_size = 0.3, random_state = 18)"
      ],
      "metadata": {
        "id": "IORnHNKKlFUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1."
      ],
      "metadata": {
        "id": "06FOHEhz7Tse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Count Vectorizer simple\n",
        "vect = CountVectorizer()\n",
        "X_dcv_tr = vect.fit_transform (X_train)\n",
        "X_dcv_te = vect.transform (X_test)\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_dcv_tr, y_train, cv=10)).describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaMcm4bEtqtI",
        "outputId": "22369c3a-846a-4979-8784-062c181106f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.976667\n",
            "std       0.004902\n",
            "min       0.969231\n",
            "25%       0.974359\n",
            "50%       0.976923\n",
            "75%       0.976923\n",
            "max       0.984615\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clrf.fit(X_dcv_tr, y_train) #Train\n",
        "y_pred_cv = clrf.predict(X_dcv_te) #Test\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_cv)"
      ],
      "metadata": {
        "id": "aIVGi2H07L1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2nBlwMTCEec",
        "outputId": "989bd25c-ecaa-46bd-cc83-b6a463c74d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9204545454545454\n",
            "AUC: 0.8727678571428572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2"
      ],
      "metadata": {
        "id": "F7r6aubu7PnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Count Vectorizer sin mayusculas, ni stop words\n",
        "vect = CountVectorizer(lowercase=False, stop_words='english')\n",
        "X_dcv_tr = vect.fit_transform (X_train)\n",
        "X_dcv_te = vect.transform (X_test)\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_dcv_tr, y_train, cv=10)).describe())"
      ],
      "metadata": {
        "id": "R31SL8VIIiQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d185a6-0b38-42e8-bd76-ee864c4724d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.976923\n",
            "std       0.007352\n",
            "min       0.966667\n",
            "25%       0.973077\n",
            "50%       0.976923\n",
            "75%       0.980769\n",
            "max       0.989744\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clrf.fit(X_dcv_tr, y_train) #Train\n",
        "y_pred_cv = clrf.predict(X_dcv_te) #Test\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_cv)"
      ],
      "metadata": {
        "id": "Ypnr5EbaJm0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXr4dIDFJqmt",
        "outputId": "e8aa7724-b92c-427e-f55e-0ad80b541209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9204545454545454\n",
            "AUC: 0.8858154104183109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3"
      ],
      "metadata": {
        "id": "KlxnfSh8JpLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Count Vectorizer con N-grams, max features, min df y max df\n",
        "vect = CountVectorizer (ngram_range=(1,2), max_features=2000, max_df=0.95, min_df=5)\n",
        "X_dcv_tr = vect.fit_transform (X_train)\n",
        "X_dcv_te = vect.transform (X_test)\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_dcv_tr, y_train, cv=10)).describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4w1HafP5koN",
        "outputId": "a78142f5-6fdd-475a-94a1-51a5d6c9a6f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.981282\n",
            "std       0.005803\n",
            "min       0.971795\n",
            "25%       0.976923\n",
            "50%       0.983333\n",
            "75%       0.986538\n",
            "max       0.987179\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clrf.fit(X_dcv_tr, y_train) #Train\n",
        "y_pred_cv = clrf.predict(X_dcv_te) #Test\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_cv)"
      ],
      "metadata": {
        "id": "P41WVZMvKAR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6cTe1_rKDFI",
        "outputId": "e760ad82-1391-4ed4-ed21-cd59a781b468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9204545454545454\n",
            "AUC: 0.9059046961325967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercise 3.2\n",
        "\n",
        "Predict target using TdidfVectorizer.\n",
        "\n",
        "use Random Forest classifier"
      ],
      "metadata": {
        "id": "pIFj3ETiIi4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['target'].map({'ham':0, 'spam':1})\n",
        "\n",
        "# split data into training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['target'], test_size = 0.3, random_state = 18)"
      ],
      "metadata": {
        "id": "jpaert_AI-6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcLYilESO7ns",
        "outputId": "63a8b0e6-32e5-472c-cd6d-c5e54a30a48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3900,)\n",
            "(1672,)\n",
            "(3900,)\n",
            "(1672,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TfIdfVectorizer simple\n",
        "tfidf = TfidfVectorizer ()\n",
        "X_train_tfidf = tfidf.fit_transform (X_train)\n",
        "X_test_tfidf = tfidf.transform (X_test)\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_train_tfidf, y_train, cv=10)).describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyDVadJTK9wl",
        "outputId": "298e957e-067b-409c-d216-926729261365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.977179\n",
            "std       0.006333\n",
            "min       0.969231\n",
            "25%       0.974359\n",
            "50%       0.975641\n",
            "75%       0.981410\n",
            "max       0.989744\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.astype('int')\n",
        "y_test.astype('int')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNcFlXEZ_GbV",
        "outputId": "d28e6c2a-802d-4091-91e1-4dd939503bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3087    0\n",
              "4062    0\n",
              "572     0\n",
              "2453    0\n",
              "4593    0\n",
              "       ..\n",
              "5405    0\n",
              "2771    0\n",
              "3373    0\n",
              "1204    0\n",
              "2400    0\n",
              "Name: target, Length: 1672, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_tfidf)"
      ],
      "metadata": {
        "id": "OUL3GeBSpl5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test_tfidf)"
      ],
      "metadata": {
        "id": "MR-pyv7u1qj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_tfidf.shape)\n",
        "print(X_test_tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbqLXMWEPGYu",
        "outputId": "75369190-e280-42d3-993f-1bb8d52b26d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3900, 7172)\n",
            "(1672, 7172)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clrf.fit(X_train_tfidf, y_train) #Train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "DJ3m2UDfNFcs",
        "outputId": "f3394cba-7ce8-4f7d-abe2-7d942d2753eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_cv = clrf.predict(X_train_tfidf)"
      ],
      "metadata": {
        "id": "TFBPSeDEqOeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_training = accuracy_score(y_train, y_pred_cv)\n",
        "print('Accuracy on Training: ', accuracy_training)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwT88pxNNIvB",
        "outputId": "66bf97e7-ef45-4eea-f030-fb1a22f2fb18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Training:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_te = clrf.predict(X_test_tfidf)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_te)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_te)\n",
        "print('Accuracy on Test: ', accuracy_test)\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIwl5PKMrFOP",
        "outputId": "a0aa7d84-7b5e-407f-e63c-61aa7dcaae0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Test:  0.97188995215311\n",
            "AUC: 0.8950892857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**TfId Complejo**"
      ],
      "metadata": {
        "id": "15FbNZqnAdKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TfIdfVectorizer Complejo\n",
        "tfidf = TfidfVectorizer (ngram_range = (1, 2), max_features = 2000, max_df = 0.95, min_df = 5)\n",
        "X_train_tfidf = tfidf.fit_transform (X_train)\n",
        "X_test_tfidf = tfidf.transform (X_test)\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_train_tfidf, y_train, cv=10)).describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_F5CR2xzTDY",
        "outputId": "8b22f766-e42e-4e6b-87a3-119e54d79a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.980769\n",
            "std       0.007074\n",
            "min       0.969231\n",
            "25%       0.974359\n",
            "50%       0.983333\n",
            "75%       0.986538\n",
            "max       0.989744\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clrf.fit(X_train_tfidf, y_train) #Train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "wwfpi0VwzwMe",
        "outputId": "944a6800-3ce3-45d2-9c76-80f4c630f6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_cv = clrf.predict(X_train_tfidf)"
      ],
      "metadata": {
        "id": "NXaICu4RzxMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_training = accuracy_score(y_train, y_pred_cv)\n",
        "print('Accuracy on Training: ', accuracy_training)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuyD8MWpz1G0",
        "outputId": "5f421fac-16b6-4c02-953d-8ea9979ddb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Training:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_te = clrf.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "x2O8y-Gnycvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_te = clrf.predict(X_test_tfidf)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_te)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_te)\n",
        "print('Accuracy on Test: ', accuracy_test)\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT8Uldtsx7I1",
        "outputId": "e41faa71-c104-4403-b78f-5680583966b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Test:  0.9736842105263158\n",
            "AUC: 0.9036725532754539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercise 3.3\n",
        "\n",
        "Predict target using CountVectorizer or TfideVectorizer.\n",
        "\n",
        "choose any classification model and justify why"
      ],
      "metadata": {
        "id": "JIlTBKIeI_bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el ejercicio 3.2 se produjo el la predicción de los 2 metodos, registrando los siguientes resultados:\n",
        "\n",
        "|Metodo | Accuracy | AUC\n",
        "|-------|-----------|-----------|\n",
        "|Count Vectorizer simple| 0.920454 | 0.87276|\n",
        "|Count Vectorizer sin mayusculas, ni stop words|0.92045 |0.88582|\n",
        "|Count Vectorizer con N-grams, max features, min df y max df| 0.920454 | 0.90590|\n",
        "|TfIdfVectorizer simple | 0.971889 | 0.89508|\n",
        "|TfIdfVectorizer con N-grams,max features, min df y max df | 0.973684 | 0.90367|\n",
        "\n",
        "Lo anterior confirma que el modelo que mejor Accuracy registro fue el **TfIdfVectorizer con N-grams** con **0.973684** y, AUC de **0.90367** por lo cual es el mejor modelo."
      ],
      "metadata": {
        "id": "kqog0D0jz_aY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercise 3.4\n",
        "\n",
        "Increase and decrece the parameters values vector_size, window and negative then predict the target.\n",
        "\n",
        "Plot the different values of the parameters with the performance of the model.\n",
        "\n",
        "Use a Random Forest classifier and classification model of your choice and justify why."
      ],
      "metadata": {
        "id": "Nv3WOp-oKK8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "parameter variation: `vector_size`"
      ],
      "metadata": {
        "id": "qI5fsc37Tg3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Vector_size = 150**"
      ],
      "metadata": {
        "id": "loDSrEfLI2cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['target'].map({'ham':0, 'spam':1})\n",
        "# split data into training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['target'], test_size = 0.3, random_state = 18)"
      ],
      "metadata": {
        "id": "7rbepGDM7KXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_test = X_test.apply(preprocess)"
      ],
      "metadata": {
        "id": "O6dVxGRK7URx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [sentence.split() for sentence in X_train]\n",
        "model = Word2Vec(sentences, vector_size=150, window=5, negative=20, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "CXsvganRGYf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(150)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])"
      ],
      "metadata": {
        "id": "ETODM3VM5HDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_train, y_train, cv=10)).describe())\n",
        "clrf.fit(X_train, y_train) #Train\n",
        "y_pred_cv = clrf.predict(X_train)\n",
        "accuracy_training = accuracy_score(y_train, y_pred_cv)\n",
        "print('Accuracy on Training: ', accuracy_training)\n",
        "y_pred_te = clrf.predict(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_te)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_te)\n",
        "print('Accuracy on Test: ', accuracy_test)\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kg3-KPX6QpJ",
        "outputId": "5f91b964-407c-4983-c8b3-55f6e6bd2082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.945385\n",
            "std       0.011147\n",
            "min       0.920513\n",
            "25%       0.944231\n",
            "50%       0.946154\n",
            "75%       0.953205\n",
            "max       0.956410\n",
            "dtype: float64\n",
            "Accuracy on Training:  1.0\n",
            "Accuracy on Test:  0.9300239234449761\n",
            "AUC: 0.7444998026835044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Vector_Size = 50**"
      ],
      "metadata": {
        "id": "5_tug-BYIvKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['target'].map({'ham':0, 'spam':1})\n",
        "# split data into training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['target'], test_size = 0.3, random_state = 18)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_test = X_test.apply(preprocess)\n",
        "\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "model = Word2Vec(sentences, vector_size=50, window=5, negative=20, min_count=1, workers=4)\n",
        "\n",
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(50)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
        "\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_train, y_train, cv=10)).describe())\n",
        "clrf.fit(X_train, y_train) #Train\n",
        "y_pred_cv = clrf.predict(X_train)\n",
        "accuracy_training = accuracy_score(y_train, y_pred_cv)\n",
        "print('Accuracy on Training: ', accuracy_training)\n",
        "y_pred_te = clrf.predict(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_te)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_te)\n",
        "print('Accuracy on Test: ', accuracy_test)\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEaXb9KEIQ2v",
        "outputId": "1a9bfe72-e7a4-4715-fe92-03721fe4e7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.942308\n",
            "std       0.008903\n",
            "min       0.925641\n",
            "25%       0.939103\n",
            "50%       0.944872\n",
            "75%       0.946154\n",
            "max       0.956410\n",
            "dtype: float64\n",
            "Accuracy on Training:  1.0\n",
            "Accuracy on Test:  0.9192583732057417\n",
            "AUC: 0.7213027821625887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Vector_Size = 200**"
      ],
      "metadata": {
        "id": "W-ktFecnJBZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['target'].map({'ham':0, 'spam':1})\n",
        "# split data into training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['target'], test_size = 0.3, random_state = 18)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_test = X_test.apply(preprocess)\n",
        "\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "model = Word2Vec(sentences, vector_size=200, window=5, negative=20, min_count=1, workers=4)\n",
        "\n",
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(200)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
        "\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_train, y_train, cv=10)).describe())\n",
        "clrf.fit(X_train, y_train) #Train\n",
        "y_pred_cv = clrf.predict(X_train)\n",
        "accuracy_training = accuracy_score(y_train, y_pred_cv)\n",
        "print('Accuracy on Training: ', accuracy_training)\n",
        "y_pred_te = clrf.predict(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_te)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_te)\n",
        "print('Accuracy on Test: ', accuracy_test)\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1GGGhSYI8FA",
        "outputId": "fa704cb3-ca1c-4626-8cf9-c1a5029fdcb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.940256\n",
            "std       0.009898\n",
            "min       0.925641\n",
            "25%       0.934615\n",
            "50%       0.939744\n",
            "75%       0.945513\n",
            "max       0.958974\n",
            "dtype: float64\n",
            "Accuracy on Training:  1.0\n",
            "Accuracy on Test:  0.9234449760765551\n",
            "AUC: 0.7161725532754538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "parameter variation: `window`"
      ],
      "metadata": {
        "id": "VVsQ2rSMTynE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**window = 10**"
      ],
      "metadata": {
        "id": "17k1ZgqMJiG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['target'].map({'ham':0, 'spam':1})\n",
        "# split data into training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['target'], test_size = 0.3, random_state = 18)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_test = X_test.apply(preprocess)\n",
        "\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "model = Word2Vec(sentences, vector_size=100, window=10, negative=20, min_count=1, workers=4)\n",
        "\n",
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(100)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
        "\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_train, y_train, cv=10)).describe())\n",
        "clrf.fit(X_train, y_train) #Train\n",
        "y_pred_cv = clrf.predict(X_train)\n",
        "accuracy_training = accuracy_score(y_train, y_pred_cv)\n",
        "print('Accuracy on Training: ', accuracy_training)\n",
        "y_pred_te = clrf.predict(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_te)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_te)\n",
        "print('Accuracy on Test: ', accuracy_test)\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "id": "JJDx3bEqTza4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869f4bd5-4d7a-402a-b270-5d4b53575a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.958462\n",
            "std       0.010865\n",
            "min       0.935897\n",
            "25%       0.952564\n",
            "50%       0.960256\n",
            "75%       0.964103\n",
            "max       0.976923\n",
            "dtype: float64\n",
            "Accuracy on Training:  1.0\n",
            "Accuracy on Test:  0.937799043062201\n",
            "AUC: 0.7904992107340174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**window = 15**"
      ],
      "metadata": {
        "id": "D9AGjusMJig6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['target'].map({'ham':0, 'spam':1})\n",
        "# split data into training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['target'], test_size = 0.3, random_state = 18)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_test = X_test.apply(preprocess)\n",
        "\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "model = Word2Vec(sentences, vector_size=100, window=15, negative=20, min_count=1, workers=4)\n",
        "\n",
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(100)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
        "\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_train, y_train, cv=10)).describe())\n",
        "clrf.fit(X_train, y_train) #Train\n",
        "y_pred_cv = clrf.predict(X_train)\n",
        "accuracy_training = accuracy_score(y_train, y_pred_cv)\n",
        "print('Accuracy on Training: ', accuracy_training)\n",
        "y_pred_te = clrf.predict(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_te)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_te)\n",
        "print('Accuracy on Test: ', accuracy_test)\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tntn5ylBJhk5",
        "outputId": "c76b8480-30d1-4d3e-9cdd-f53b165aa8f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.960513\n",
            "std       0.010342\n",
            "min       0.933333\n",
            "25%       0.958974\n",
            "50%       0.962821\n",
            "75%       0.966026\n",
            "max       0.969231\n",
            "dtype: float64\n",
            "Accuracy on Training:  1.0\n",
            "Accuracy on Test:  0.9503588516746412\n",
            "AUC: 0.8392610497237569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "parameter variation: `negative`"
      ],
      "metadata": {
        "id": "HXwksOx_Tz70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**negative = 10**"
      ],
      "metadata": {
        "id": "rTENdsm_Ku3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['target'].map({'ham':0, 'spam':1})\n",
        "# split data into training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['target'], test_size = 0.3, random_state = 18)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_test = X_test.apply(preprocess)\n",
        "\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "model = Word2Vec(sentences, vector_size=100, window=15, negative=10, min_count=1, workers=4)\n",
        "\n",
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(100)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
        "\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_train, y_train, cv=10)).describe())\n",
        "clrf.fit(X_train, y_train) #Train\n",
        "y_pred_cv = clrf.predict(X_train)\n",
        "accuracy_training = accuracy_score(y_train, y_pred_cv)\n",
        "print('Accuracy on Training: ', accuracy_training)\n",
        "y_pred_te = clrf.predict(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_te)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_te)\n",
        "print('Accuracy on Test: ', accuracy_test)\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "id": "bzJP7O9qT0ZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4986fe-36af-41a3-edee-6c39528c02ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.958974\n",
            "std       0.008800\n",
            "min       0.943590\n",
            "25%       0.956410\n",
            "50%       0.957692\n",
            "75%       0.965385\n",
            "max       0.971795\n",
            "dtype: float64\n",
            "Accuracy on Training:  1.0\n",
            "Accuracy on Test:  0.9467703349282297\n",
            "AUC: 0.816433997632202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**negative = 15**"
      ],
      "metadata": {
        "id": "VjsR-DTMLE-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['target'].map({'ham':0, 'spam':1})\n",
        "# split data into training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['target'], test_size = 0.3, random_state = 18)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_test = X_test.apply(preprocess)\n",
        "\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "model = Word2Vec(sentences, vector_size=100, window=15, negative=15, min_count=1, workers=4)\n",
        "\n",
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(100)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
        "\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_train, y_train, cv=10)).describe())\n",
        "clrf.fit(X_train, y_train) #Train\n",
        "y_pred_cv = clrf.predict(X_train)\n",
        "accuracy_training = accuracy_score(y_train, y_pred_cv)\n",
        "print('Accuracy on Training: ', accuracy_training)\n",
        "y_pred_te = clrf.predict(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_te)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_te)\n",
        "print('Accuracy on Test: ', accuracy_test)\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W85HqNjBLBYi",
        "outputId": "8cf7c289-0a42-4701-b612-5f0bd22bcb32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.962051\n",
            "std       0.008615\n",
            "min       0.941026\n",
            "25%       0.959615\n",
            "50%       0.962821\n",
            "75%       0.967949\n",
            "max       0.971795\n",
            "dtype: float64\n",
            "Accuracy on Training:  1.0\n",
            "Accuracy on Test:  0.9521531100478469\n",
            "AUC: 0.8384101223362274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**negative = 25**"
      ],
      "metadata": {
        "id": "OOfq_orVLoV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['target'].map({'ham':0, 'spam':1})\n",
        "# split data into training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['target'], test_size = 0.3, random_state = 18)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_test = X_test.apply(preprocess)\n",
        "\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "model = Word2Vec(sentences, vector_size=100, window=15, negative=25, min_count=1, workers=4)\n",
        "\n",
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(100)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
        "\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_train, y_train, cv=10)).describe())\n",
        "clrf.fit(X_train, y_train) #Train\n",
        "y_pred_cv = clrf.predict(X_train)\n",
        "accuracy_training = accuracy_score(y_train, y_pred_cv)\n",
        "print('Accuracy on Training: ', accuracy_training)\n",
        "y_pred_te = clrf.predict(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_te)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_te)\n",
        "print('Accuracy on Test: ', accuracy_test)\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pZ8FkHOLkBx",
        "outputId": "cbacd6bb-0228-4bbd-8417-87a7b3b72e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.960000\n",
            "std       0.007852\n",
            "min       0.941026\n",
            "25%       0.957051\n",
            "50%       0.961538\n",
            "75%       0.964103\n",
            "max       0.969231\n",
            "dtype: float64\n",
            "Accuracy on Training:  1.0\n",
            "Accuracy on Test:  0.9473684210526315\n",
            "AUC: 0.8243266574585635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running best parameters `vector_size, window and negative` (All at the same time)"
      ],
      "metadata": {
        "id": "E278ShYhT-Vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El mejor modelo quedo con los siguientes parametros los cuales indican los mejores rendimientos:\n",
        "\n",
        "-------------------------\n",
        "\n",
        "* vector_size=100\n",
        "* window=15\n",
        "* negative=15"
      ],
      "metadata": {
        "id": "OG6P3xDdMOTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['target'].map({'ham':0, 'spam':1})\n",
        "# split data into training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['target'], test_size = 0.3, random_state = 18)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_test = X_test.apply(preprocess)\n",
        "\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "model = Word2Vec(sentences, vector_size=100, window=15, negative=15, min_count=1, workers=4)\n",
        "\n",
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(100)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
        "\n",
        "clrf = RandomForestClassifier()\n",
        "print(pd.Series(cross_val_score(clrf, X_train, y_train, cv=10)).describe())\n",
        "clrf.fit(X_train, y_train) #Train\n",
        "y_pred_cv = clrf.predict(X_train)\n",
        "accuracy_training = accuracy_score(y_train, y_pred_cv)\n",
        "print('Accuracy on Training: ', accuracy_training)\n",
        "y_pred_te = clrf.predict(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_te)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_te)\n",
        "print('Accuracy on Test: ', accuracy_test)\n",
        "print('AUC:', auc(fpr, tpr))"
      ],
      "metadata": {
        "id": "D3AD321hUDaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcdae4ce-2f57-41a4-a879-4e4578f845b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    10.000000\n",
            "mean      0.958462\n",
            "std       0.010662\n",
            "min       0.933333\n",
            "25%       0.956410\n",
            "50%       0.958974\n",
            "75%       0.964744\n",
            "max       0.971795\n",
            "dtype: float64\n",
            "Accuracy on Training:  1.0\n",
            "Accuracy on Test:  0.9491626794258373\n",
            "AUC: 0.8253625690607734\n"
          ]
        }
      ]
    }
  ]
}